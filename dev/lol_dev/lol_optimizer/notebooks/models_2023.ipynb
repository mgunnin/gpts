{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/oracle-devrel/leagueoflegends-optimizer/raw/livelabs/images/structure_2023.webp)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we haven't already, we need to install Python dependencies for our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r /Users/mattgunnin/Sites/AI/00_Active/leagueoflegends-optimizer/deps/requirements_2023.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation & Exploration\n",
    "\n",
    "First, we will read the data from our previously-exported CSV file. Then, we will split the CSV file into train and test so that we can use different CSV files for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from autogluon.tabular import TabularDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "print(plt.style.available)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"/Users/mattgunnin/Sites/AI/20_GPT/gpt_lolcoach/performance_report.csv\",\n",
    "    skipinitialspace=True,\n",
    "    index_col=[0]\n",
    ")\n",
    "# special train-test split into two equally shaped dataframes\n",
    "df['split'] = np.random.randn(df.shape[0], 1)\n",
    "\n",
    "msk = np.random.rand(len(df)) <= 0.85 # I create a boolean mask so my resulting dataframe is easily filterable,\n",
    "# like this:\n",
    "train = df[msk]\n",
    "test = df[~msk]\n",
    "\n",
    "# put into files for future use\n",
    "train.to_csv('train.csv')\n",
    "test.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['calculated_player_performance'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['calculated_player_performance', 'f1', 'f2', 'f3']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.hist(column=['assists', 'baronKills', 'champExperience',\n",
    "                   'deaths', 'detectorWardsPlaced', 'dragonKills', 'goldEarned', \n",
    "                   'goldSpent', 'kills', 'largestCriticalStrike', 'largestMultiKill', 'largestKillingSpree',\n",
    "                   'doubleKills', 'tripleKills', 'quadraKills', 'pentaKills', 'totalDamageDealt', 'totalDamageTaken', 'visionScore',\n",
    "                   'wardsKilled', 'turretKills', 'duration', 'f1', 'f2', 'f3'],\n",
    "           figsize=(30, 30),\n",
    "           bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train\n",
    "\n",
    "# We will create a first model with no f's, and removing the auxiliary 'split' column (which was used for train-test splitting).\n",
    "X = X.drop(columns=['f1', 'f2', 'f3', 'f4', 'f5', 'split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a TabularDataset object (it's a Pandas Dataframe with more powers)\n",
    "train_data = TabularDataset(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the kind of data we can expect:\n",
    "train_data.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_available_graphs = ['stacked_histogram', 'density', 'box_plot', 'scatter_patterns']\n",
    "\n",
    "df1 = X.cumsum()\n",
    "print('Calculated cumulative sum of df')\n",
    "ax = df1.plot()\n",
    "print('Got ax')\n",
    "\n",
    "for x in range(len(list_available_graphs)):\n",
    "    print('Creating multiple visualizations...')\n",
    "    \n",
    "    ax = df1.plot()\n",
    "    if x == 'stacked_histogram':\n",
    "        ax = X.plot.hist(bins=25, stacked=True) # for stacked histogram plot\n",
    "    elif x == 'density':\n",
    "        ax = X.plot.kde() # for a density plot\n",
    "    elif x == 'box_plot':\n",
    "        X.plot.box(vert=False) # for a box plot\n",
    "    elif x == 'scatter_patterns':\n",
    "        ax = X.plot.scatter(x='x', y='y') # for comparing scatter patterns between variables x and y\n",
    "\n",
    "\n",
    "    # from here down – standard plot output\n",
    "    ax.set_title('Visualization {}'.format(x))\n",
    "    ax.set_xlabel('X Axis')\n",
    "    ax.set_ylabel('Y Axis')\n",
    "    fig = ax.figure\n",
    "    fig.set_size_inches(8, 3)\n",
    "    fig.tight_layout(pad=1)\n",
    "    fig.savefig('filename_{}.png'.format(x), dpi=125)\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Now that we've seen the shape of our dataset and we have the variable we want to predict (in this case, calculated_player_performance), we train as many models as possible for 10 minutes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "predictor = TabularPredictor(label='calculated_player_performance',\n",
    "                             verbosity=2,\n",
    "                            problem_type='regression',\n",
    "                            path='./player_performance_models',\n",
    "                            ).fit(train_data, time_limit=10*60, presets='medium_quality')\n",
    "# https://auto.gluon.ai/0.5.2/tutorials/tabular_prediction/tabular-quickstart.html#presets # medium_quality, good_quality, high_quality, best_quality\n",
    "# Among the three presets, medium_quality has the smallest model size. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We display a leaderboard of the best trained models ordered by decreasing RMSE \n",
    "predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.fit_summary(show_plot=True)\n",
    "# this show_plot=True will generate a HTML file with detailed infromation about each model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting with ensemble of models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing\n",
    "\n",
    "Now that we have our first set of models trained, let's demonstrate how to make predictions on new data. Since we previously created test.csv, we can use the data that's in there already.¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TabularDataset(test)\n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make all predictions in parallel\n",
    "predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the class probabilities for classification -> since this is a regression problem, probabilities are the same.\n",
    "# predictor.predict_proba(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE, MAE, RMSE, and R-Squared metrics are mainly used to evaluate the prediction error rates and model performance in regression analysis.\n",
    "- MAE (Mean absolute error) represents the difference between the original and predicted values extracted by averaged the absolute difference over the data set.\n",
    "- MSE (Mean Squared Error) represents the difference between the original and predicted values extracted by squared the average difference over the data set.\n",
    "- RMSE (Root Mean Squared Error) is the error rate by the square root of MSE.\n",
    "- R-squared (Coefficient of determination) represents the coefficient of how well the values fit compared to the original values.\n",
    "    The value from 0 to 1 interpreted as percentages. The higher the value is, the better the model is.\n",
    "- The Pearson correlation coefficient is a descriptive statistic, meaning that it summarizes the characteristics of a dataset\n",
    "    Specifically, it describes the strength and direction of the linear relationship between two quantitative variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate various metrics, it needs test_data to have the label column\n",
    "predictor.evaluate(test_data)\n",
    "\n",
    "# This helps us evaluate how well our model behaves\n",
    "'''\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the importance of each feature. -> How much it affects the decision making of our models\n",
    "predictor.feature_importance(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting with one only model\n",
    "Even if we're creating several models, we can choose to use our favorite; even though the best performing models are usually weighted ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of string names\n",
    "models = predictor.get_model_names()\n",
    "# Predict with the 2nd model. Both predict_proba and evaluate also accept the model argument\n",
    "predictor.predict(test_data, model=models[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Win Predictor\n",
    "\n",
    "Now that we have a model that successfully predicts each player's performance, we will create a second group of models to predict the binary variable 'win'. This is just something extra, as the other model would be sufficient to determine how well you're performing, but I decided to provide as many relatively-useful models as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "predictor = TabularPredictor(label='win',\n",
    "                             verbosity=2,\n",
    "                            problem_type='binary', # ‘binary’, ‘multiclass’, ‘regression’, ‘quantile’\n",
    "                            path='./winner_models',\n",
    "                            ).fit(train_data, time_limit=10*60, presets='medium_quality') # https://auto.gluon.ai/0.5.2/tutorials/tabular_prediction/tabular-quickstart.html#presets # medium_quality, good_quality, high_quality, best_quality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.leaderboard()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/oracle-devrel/leagueoflegends-optimizer/raw/livelabs/images/example_ensemble.png)\n",
    "> **Note**: this is an example of an weighted ensemble model, in which decisions are taken using a technique called **bagging**: every model makes a prediction, and the best models will weigh more upon the final decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictor.path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference\n",
    "\n",
    "In this small chapter, we learn how to import already-trained models to this notebook (or any Python script) from our local storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del predictor\n",
    "\n",
    "predictor = TabularPredictor.load('./winner_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(test_data.iloc[0:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Client API-Compatible Model\n",
    "\n",
    "Now, we build a model compatible with the data that Live Client API provides. \n",
    "\n",
    "To give you an idea of the type of data present in this API, here's are some images of the full data:\n",
    "\n",
    "![](https://github.com/oracle-devrel/leagueoflegends-optimizer/raw/livelabs/images/live_client_1.PNG)\n",
    "\n",
    "This data was the one we primarily used last year: having information from the player on their current stats, we built a model that considered the player's stats and returned a winning probability. However, since stats aren't as important in our models (as observed by predictor.feature_importance(test_data)), the model had about 65-70% accuracy only.\n",
    "\n",
    "However, we're interested in also getting the player level from this structure.\n",
    "\n",
    "![](https://github.com/oracle-devrel/leagueoflegends-optimizer/raw/livelabs/images/live_client_2.PNG)\n",
    "\n",
    "From this `gameData` structure, we get the `gameTime` variable to get player statistics per minute.\n",
    "\n",
    "![](https://github.com/oracle-devrel/leagueoflegends-optimizer/raw/livelabs/images/live_client_3.PNG)\n",
    "\n",
    "And, from this last object, we will extract:\n",
    "- Kills\n",
    "- Deaths\n",
    "- Assists\n",
    "\n",
    "And compute: \n",
    "- Kills + assists / gameTime ==> kills + assists ratio ==> f2\n",
    "- Deaths / gameTime ==> death ratio ==> f1\n",
    "- xp / gameTime ==> xp per min ==> f3\n",
    "\n",
    "In our dataset, we also had two other variables that I was hoping I could also calculate with Live Client API data, but these variables weren't possible to accurately calculate:\n",
    "- f4, which represented the total amount of damage per minute, wasn't present in the Live Client API in any field\n",
    "- f5, which represented the total amount of gold per minute, wasn't either. You can only extract the **current** amount of gold, which doesn't add any real value to the model.\n",
    "\n",
    "\n",
    "\n",
    "So, the idea now is to create a model that, given f1, f2 and f3, and the champion name, is **able to predict any player's performance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_limit = 10*60  # train various models for ~10 min\n",
    "\n",
    "# dataset f1...f5\n",
    "'''\n",
    "    'f1': deaths_per_min, - present\n",
    "    'f2': k_a_per_min, - present\n",
    "    'f3': level_per_min, - present\n",
    "    'f4': total_damage_per_min, - NOT present\n",
    "    'f5': gold_per_min, - NOT present\n",
    "'''\n",
    "\n",
    "# try a model with only f1...f3 as features and player performance as target\n",
    "\n",
    "X = train\n",
    "X = X[['championName', 'f1', 'f2', 'f3', 'calculated_player_performance']]\n",
    "# This model will have 4 inputs and 1 output: calculated_player_performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We instantiate the predictor and start fitting the model with our data.\n",
    "from autogluon.tabular import TabularPredictor\n",
    "predictor = TabularPredictor(label='calculated_player_performance',\n",
    "                             verbosity=2,\n",
    "                            problem_type='regression', # ‘binary’, ‘multiclass’, ‘regression’, ‘quantile’\n",
    "                            path='./live_model_1',\n",
    "                            ).fit(X, time_limit=time_limit, presets='medium_quality',\n",
    "                                 #hyperparameters=hyperparameters,\n",
    "                                 #hyperparameter_tune_kwargs=hyperparameter_tune_kwargs\n",
    "                                ) # https://auto.gluon.ai/0.5.2/tutorials/tabular_prediction/tabular-quickstart.html#presets # medium_quality, good_quality, high_quality, best_quality \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how well it went\n",
    "predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = test[['championName', 'f1', 'f2', 'f3', 'calculated_player_performance']]\n",
    "\n",
    "predictor.feature_importance(new_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lol-optimizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
